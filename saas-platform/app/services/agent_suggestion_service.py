"""
å®¢æœè¯æœ¯æ¨èæœåŠ¡

å®æ—¶åˆ†æç”¨æˆ·é—®é¢˜ï¼Œä¸ºå®¢æœäººå‘˜æ¨èåˆé€‚çš„å›å¤è¯æœ¯å’Œå¤„ç†å»ºè®®
"""
import logging
from typing import Optional, Dict, Any, List
from uuid import UUID
from datetime import datetime, timedelta

from sqlalchemy.ext.asyncio import AsyncSession

from app.services.llm.base_provider import (
    BaseLLMProvider, 
    LLMConfig, 
    LLMMessage, 
    LLMProviderError
)
from app.services.llm.openai_provider import OpenAIProvider
from app.services.context_manager import ContextManager
from app.services.message_service import MessageService
from app.services.session_service import SessionService
from app.schemas.message import MessageRead

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class AgentSuggestionService:
    """å®¢æœè¯æœ¯æ¨èæœåŠ¡"""
    
    def __init__(self, db: AsyncSession):
        """
        åˆå§‹åŒ–å®¢æœè¯æœ¯æ¨èæœåŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
        """
        self.db = db
        self.context_manager = ContextManager(db)
        self.message_service = MessageService(db)
        self.session_service = SessionService(db)
        
        # LLMæä¾›å•†ç¼“å­˜
        self._providers: Dict[str, BaseLLMProvider] = {}
        
        # è¯æœ¯æ¨¡æ¿ç¼“å­˜
        self._templates_cache: Dict[str, List[Dict[str, Any]]] = {}
    
    async def get_reply_suggestions(
        self,
        session_id: UUID,
        tenant_id: UUID,
        user_message: str,
        suggestion_count: int = 3,
        suggestion_type: str = "general"
    ) -> Dict[str, Any]:
        """
        è·å–å›å¤å»ºè®®
        
        Args:
            session_id: ä¼šè¯ID
            tenant_id: ç§Ÿæˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            suggestion_count: å»ºè®®æ•°é‡
            suggestion_type: å»ºè®®ç±»å‹ ("general", "professional", "empathetic", "solution")
            
        Returns:
            Dict[str, Any]: å›å¤å»ºè®®ä¿¡æ¯
            
        Raises:
            ValueError: å‚æ•°æ— æ•ˆ
            LLMProviderError: LLMè°ƒç”¨å¤±è´¥
        """
        try:
            # éªŒè¯è¾“å…¥
            if not user_message.strip():
                raise ValueError("User message cannot be empty")
            
            # è·å–ä¼šè¯ä¸Šä¸‹æ–‡
            session_context = await self._get_session_context(session_id, tenant_id)
            
            # åˆ†æç”¨æˆ·æ¶ˆæ¯
            message_analysis = await self._analyze_user_message(user_message, tenant_id)
            
            # ç”ŸæˆåŸºäºAIçš„å»ºè®®
            ai_suggestions = await self._generate_ai_suggestions(
                user_message=user_message,
                session_context=session_context,
                message_analysis=message_analysis,
                suggestion_count=suggestion_count,
                suggestion_type=suggestion_type,
                tenant_id=tenant_id
            )
            
            # è·å–æ¨¡æ¿åŒ–å»ºè®®
            template_suggestions = await self._get_template_suggestions(
                message_analysis=message_analysis,
                tenant_id=tenant_id,
                suggestion_count=min(2, suggestion_count)
            )
            
            # ç”Ÿæˆå¿«é€Ÿå›å¤é€‰é¡¹
            quick_replies = await self._generate_quick_replies(
                message_analysis=message_analysis,
                tenant_id=tenant_id
            )
            
            # ç”Ÿæˆå¤„ç†å»ºè®®
            handling_tips = await self._generate_handling_tips(
                message_analysis=message_analysis,
                session_context=session_context,
                tenant_id=tenant_id
            )
            
            # æ„å»ºå®Œæ•´çš„å»ºè®®å“åº”
            suggestion_response = {
                "session_id": str(session_id),
                "tenant_id": str(tenant_id),
                "user_message": user_message,
                "generated_at": datetime.utcnow().isoformat(),
                "message_analysis": message_analysis,
                "ai_suggestions": ai_suggestions,
                "template_suggestions": template_suggestions,
                "quick_replies": quick_replies,
                "handling_tips": handling_tips,
                "suggestion_metadata": {
                    "suggestion_type": suggestion_type,
                    "suggestion_count": suggestion_count,
                    "context_length": len(session_context.get("recent_messages", []))
                }
            }
            
            logger.info("reply_suggestions_generated", 
                       session_id=session_id,
                       tenant_id=tenant_id,
                       suggestion_type=suggestion_type,
                       ai_count=len(ai_suggestions),
                       template_count=len(template_suggestions))
            
            return suggestion_response
            
        except Exception as e:
            logger.error("reply_suggestions_generation_error", 
                        session_id=session_id,
                        tenant_id=tenant_id,
                        error=str(e))
            raise
    
    async def _get_session_context(
        self, 
        session_id: UUID, 
        tenant_id: UUID
    ) -> Dict[str, Any]:
        """
        è·å–ä¼šè¯ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            session_id: ä¼šè¯ID
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            Dict[str, Any]: ä¼šè¯ä¸Šä¸‹æ–‡
        """
        try:
            # è·å–ä¼šè¯åŸºæœ¬ä¿¡æ¯
            session = await self.session_service.get_session(session_id, tenant_id)
            if not session:
                return {"error": "session_not_found"}
            
            # è·å–æœ€è¿‘çš„æ¶ˆæ¯
            recent_messages_result = await self.message_service.get_session_messages(
                session_id=session_id,
                tenant_id=tenant_id,
                page=1,
                page_size=10,
                message_type=None
            )
            
            recent_messages = recent_messages_result.items
            
            # è®¡ç®—ä¼šè¯ç»Ÿè®¡ä¿¡æ¯
            user_message_count = len([msg for msg in recent_messages if msg.message_type == "user"])
            agent_message_count = len([msg for msg in recent_messages if msg.message_type == "agent"])
            
            # è®¡ç®—ä¼šè¯æ—¶é•¿
            session_duration = 0
            if recent_messages:
                start_time = session.created_at
                latest_time = max(msg.created_at for msg in recent_messages)
                session_duration = (latest_time - start_time).total_seconds() / 60
            
            return {
                "session_info": {
                    "id": str(session.id),
                    "user_id": session.user_id,
                    "status": session.status,
                    "created_at": session.created_at.isoformat(),
                    "duration_minutes": round(session_duration, 2)
                },
                "message_stats": {
                    "total_messages": len(recent_messages),
                    "user_messages": user_message_count,
                    "agent_messages": agent_message_count
                },
                "recent_messages": [
                    {
                        "content": msg.content,
                        "type": msg.message_type,
                        "created_at": msg.created_at.isoformat()
                    }
                    for msg in recent_messages
                ]
            }
            
        except Exception as e:
            logger.error("get_session_context_error", 
                        session_id=session_id,
                        error=str(e))
            return {"error": str(e)}
    
    async def _analyze_user_message(
        self, 
        user_message: str, 
        tenant_id: UUID
    ) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·æ¶ˆæ¯
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            Dict[str, Any]: æ¶ˆæ¯åˆ†æç»“æœ
        """
        try:
            provider = await self._get_llm_provider(tenant_id)
            
            # æ„å»ºåˆ†ææç¤ºè¯
            analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯çš„ç‰¹å¾ï¼š

ç”¨æˆ·æ¶ˆæ¯ï¼š{user_message}

è¯·åˆ†æï¼š
1. æ¶ˆæ¯ç±»å‹ (question, complaint, compliment, request, urgent, other)
2. æƒ…ç»ªçŠ¶æ€ (positive, negative, neutral, frustrated, anxious)
3. ç´§æ€¥ç¨‹åº¦ (low, medium, high, critical)
4. ä¸»è¦å…³é”®è¯
5. æ„å›¾åˆ†ç±» (inquiry, support, complaint, purchase, refund, technical, other)

è¯·ç®€è¦å›å¤åˆ†æç»“æœã€‚"""
            
            context = [LLMMessage(
                role="system", 
                content="ä½ æ˜¯ä¸“ä¸šçš„å®¢æœæ¶ˆæ¯åˆ†æå¸ˆã€‚"
            )]
            context.append(LLMMessage(role="user", content=analysis_prompt))
            
            response = await provider.generate_response(context)
            
            # ç®€åŒ–çš„åˆ†æç»“æœ
            return {
                "message_type": "question",
                "emotion": "neutral",
                "urgency": "low",
                "keywords": user_message.split()[:3],
                "intent": "inquiry",
                "summary": response.content[:100] + "..." if len(response.content) > 100 else response.content
            }
            
        except Exception as e:
            logger.error("analyze_user_message_error", error=str(e))
            return {
                "message_type": "other",
                "emotion": "neutral",
                "urgency": "low",
                "keywords": [],
                "intent": "inquiry",
                "summary": "åˆ†æå¤±è´¥"
            }
    
    async def _generate_ai_suggestions(
        self,
        user_message: str,
        session_context: Dict[str, Any],
        message_analysis: Dict[str, Any],
        suggestion_count: int,
        suggestion_type: str,
        tenant_id: UUID
    ) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆAIå»ºè®®
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
            message_analysis: æ¶ˆæ¯åˆ†æ
            suggestion_count: å»ºè®®æ•°é‡
            suggestion_type: å»ºè®®ç±»å‹
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            List[Dict[str, Any]]: AIå»ºè®®åˆ—è¡¨
        """
        try:
            provider = await self._get_llm_provider(tenant_id)
            
            # æ„å»ºå»ºè®®ç”Ÿæˆæç¤ºè¯
            suggestion_prompt = f"""åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯ï¼Œç”Ÿæˆ{suggestion_count}ä¸ªä¸“ä¸šçš„å®¢æœå›å¤å»ºè®®ï¼š

ç”¨æˆ·æ¶ˆæ¯ï¼š{user_message}

è¯·ç”Ÿæˆ{suggestion_count}ä¸ªä¸åŒé£æ ¼çš„å›å¤å»ºè®®ï¼š
1. æ­£å¼ä¸“ä¸šçš„å›å¤
2. å‹å¥½äº²åˆ‡çš„å›å¤
3. ç®€æ´ç›´æ¥çš„å›å¤

æ¯ä¸ªå»ºè®®è¦ä¸“ä¸šã€å‹å¥½ã€æœ‰å¸®åŠ©ã€‚"""
            
            context = [LLMMessage(
                role="system", 
                content="ä½ æ˜¯èµ„æ·±çš„å®¢æœåŸ¹è®­å¸ˆï¼Œä¸ºå®¢æœäººå‘˜æä¾›ä¸“ä¸šçš„å›å¤å»ºè®®ã€‚"
            )]
            context.append(LLMMessage(role="user", content=suggestion_prompt))
            
            response = await provider.generate_response(context)
            
            # è§£æå»ºè®®ï¼ˆç®€åŒ–ç‰ˆï¼‰
            suggestions_text = response.content.strip().split('\n')
            suggestions = []
            
            for i, text in enumerate(suggestions_text[:suggestion_count]):
                if text.strip():
                    suggestions.append({
                        "content": text.strip(),
                        "type": f"style_{i+1}",
                        "source": "ai_generated"
                    })
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå»ºè®®
            if not suggestions:
                suggestions = [{
                    "content": "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼Œæˆ‘ä¼šå°½å¿«ä¸ºæ‚¨å¤„ç†ã€‚",
                    "type": "default",
                    "source": "fallback"
                }]
            
            return suggestions
            
        except Exception as e:
            logger.error("generate_ai_suggestions_error", error=str(e))
            return [{
                "content": "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼Œæˆ‘æ¥ä¸ºæ‚¨å¤„ç†ã€‚",
                "type": "default",
                "source": "fallback"
            }]
    
    async def _get_template_suggestions(
        self,
        message_analysis: Dict[str, Any],
        tenant_id: UUID,
        suggestion_count: int = 2
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ¨¡æ¿åŒ–å»ºè®®
        
        Args:
            message_analysis: æ¶ˆæ¯åˆ†æç»“æœ
            tenant_id: ç§Ÿæˆ·ID
            suggestion_count: å»ºè®®æ•°é‡
            
        Returns:
            List[Dict[str, Any]]: æ¨¡æ¿å»ºè®®åˆ—è¡¨
        """
        # è·å–æˆ–åˆ›å»ºæ¨¡æ¿åº“
        templates = await self._get_template_library(tenant_id)
        
        # æ ¹æ®æ¶ˆæ¯åˆ†æåŒ¹é…åˆé€‚çš„æ¨¡æ¿
        matched_templates = []
        
        message_type = message_analysis.get('message_type', 'other')
        emotion = message_analysis.get('emotion', 'neutral')
        intent = message_analysis.get('intent', 'inquiry')
        
        # æ¨¡æ¿åŒ¹é…é€»è¾‘
        for template in templates:
            template_types = template.get('applicable_types', [])
            template_emotions = template.get('applicable_emotions', [])
            template_intents = template.get('applicable_intents', [])
            
            score = 0
            if message_type in template_types:
                score += 3
            if emotion in template_emotions:
                score += 2
            if intent in template_intents:
                score += 1
            
            if score > 0:
                matched_templates.append((score, template))
        
        # æŒ‰åŒ¹é…åˆ†æ•°æ’åºå¹¶è¿”å›å‰å‡ ä¸ª
        matched_templates.sort(key=lambda x: x[0], reverse=True)
        
        return [
            {
                "content": template[1]['content'],
                "scenario": template[1]['scenario'],
                "expected_effect": template[1]['effect'],
                "source": "template",
                "template_id": template[1]['id']
            }
            for template in matched_templates[:suggestion_count]
        ]
    
    async def _get_template_library(self, tenant_id: UUID) -> List[Dict[str, Any]]:
        """
        è·å–æ¨¡æ¿åº“
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            List[Dict[str, Any]]: æ¨¡æ¿åˆ—è¡¨
        """
        cache_key = str(tenant_id)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._templates_cache:
            return self._templates_cache[cache_key]
        
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“åŠ è½½ç§Ÿæˆ·çš„è‡ªå®šä¹‰æ¨¡æ¿
        # æš‚æ—¶è¿”å›é»˜è®¤æ¨¡æ¿
        default_templates = [
            {
                "id": "greeting_1",
                "content": "æ‚¨å¥½ï¼æ„Ÿè°¢æ‚¨è”ç³»æˆ‘ä»¬ï¼Œæˆ‘æ˜¯æ‚¨çš„ä¸“å±å®¢æœï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "scenario": "åˆæ¬¡æ¥è§¦",
                "effect": "å»ºç«‹å‹å¥½çš„ç¬¬ä¸€å°è±¡",
                "applicable_types": ["question", "other"],
                "applicable_emotions": ["neutral", "positive"],
                "applicable_intents": ["inquiry", "support"]
            },
            {
                "id": "complaint_1",
                "content": "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥äº†ä¸ä¾¿ï¼Œæˆ‘å®Œå…¨ç†è§£æ‚¨çš„æ„Ÿå—ã€‚è®©æˆ‘ç«‹å³ä¸ºæ‚¨æŸ¥çœ‹å…·ä½“æƒ…å†µå¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚",
                "scenario": "å¤„ç†æŠ•è¯‰",
                "effect": "è¡¨è¾¾åŒç†å¿ƒï¼Œç¼“è§£ç”¨æˆ·æƒ…ç»ª",
                "applicable_types": ["complaint"],
                "applicable_emotions": ["negative", "frustrated"],
                "applicable_intents": ["complaint", "support"]
            },
            {
                "id": "technical_1",
                "content": "æˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†è¯´æ˜æ“ä½œæ­¥éª¤ã€‚ä¸ºäº†ç¡®ä¿å‡†ç¡®æ€§ï¼Œå»ºè®®æ‚¨å¯ä»¥æˆªå›¾æˆ–å½•å±ï¼Œè¿™æ ·æˆ‘èƒ½æ›´å¥½åœ°ä¸ºæ‚¨æŒ‡å¯¼ã€‚",
                "scenario": "æŠ€æœ¯æ”¯æŒ",
                "effect": "æä¾›ä¸“ä¸šæŠ€æœ¯ååŠ©",
                "applicable_types": ["question"],
                "applicable_emotions": ["neutral"],
                "applicable_intents": ["technical", "support"]
            }
        ]
        
        # ç¼“å­˜æ¨¡æ¿
        self._templates_cache[cache_key] = default_templates
        
        return default_templates
    
    async def _generate_quick_replies(
        self,
        message_analysis: Dict[str, Any],
        tenant_id: UUID
    ) -> List[str]:
        """
        ç”Ÿæˆå¿«é€Ÿå›å¤é€‰é¡¹
        
        Args:
            message_analysis: æ¶ˆæ¯åˆ†æç»“æœ
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            List[str]: å¿«é€Ÿå›å¤é€‰é¡¹
        """
        # åŸºäºæ¶ˆæ¯åˆ†æç”Ÿæˆå¿«é€Ÿå›å¤
        quick_replies = []
        
        message_type = message_analysis.get('message_type', 'other')
        emotion = message_analysis.get('emotion', 'neutral')
        urgency = message_analysis.get('urgency', 'low')
        
        # æ ¹æ®ä¸åŒæƒ…å†µç”Ÿæˆå¿«é€Ÿå›å¤
        if emotion == 'negative':
            quick_replies.extend([
                "æˆ‘ç†è§£æ‚¨çš„æ‹…å¿§ï¼Œè®©æˆ‘ç«‹å³ä¸ºæ‚¨å¤„ç†",
                "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿",
                "æ‚¨çš„åé¦ˆå¯¹æˆ‘ä»¬å¾ˆé‡è¦"
            ])
        elif emotion == 'positive':
            quick_replies.extend([
                "æ„Ÿè°¢æ‚¨çš„è®¤å¯",
                "å¾ˆé«˜å…´èƒ½å¸®åŠ©åˆ°æ‚¨",
                "è°¢è°¢æ‚¨çš„è€å¿ƒ"
            ])
        
        if urgency == 'high':
            quick_replies.extend([
                "æˆ‘é©¬ä¸Šä¸ºæ‚¨ä¼˜å…ˆå¤„ç†",
                "è¿™ç¡®å®æ¯”è¾ƒç´§æ€¥ï¼Œè®©æˆ‘ç«‹å³è·Ÿè¿›"
            ])
        
        if message_type == 'question':
            quick_replies.extend([
                "è®©æˆ‘ä¸ºæ‚¨æŸ¥è¯¢ä¸€ä¸‹",
                "æˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†è¯´æ˜",
                "è¯·ç¨ç­‰ï¼Œæˆ‘æŸ¥çœ‹ç›¸å…³ä¿¡æ¯"
            ])
        
        # é€šç”¨å¿«é€Ÿå›å¤
        quick_replies.extend([
            "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†",
            "è¯·æ‚¨ç¨ç­‰ç‰‡åˆ»",
            "è¿˜æœ‰å…¶ä»–éœ€è¦å¸®åŠ©çš„å—ï¼Ÿ"
        ])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_replies = list(set(quick_replies))
        return unique_replies[:6]  # æœ€å¤šè¿”å›6ä¸ªå¿«é€Ÿå›å¤
    
    async def _generate_handling_tips(
        self,
        message_analysis: Dict[str, Any],
        session_context: Dict[str, Any],
        tenant_id: UUID
    ) -> List[str]:
        """
        ç”Ÿæˆå¤„ç†å»ºè®®
        
        Args:
            message_analysis: æ¶ˆæ¯åˆ†æç»“æœ
            session_context: ä¼šè¯ä¸Šä¸‹æ–‡
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            List[str]: å¤„ç†å»ºè®®åˆ—è¡¨
        """
        tips = []
        
        emotion = message_analysis.get('emotion', 'neutral')
        urgency = message_analysis.get('urgency', 'low')
        complexity = message_analysis.get('complexity', 'moderate')
        message_type = message_analysis.get('message_type', 'other')
        
        # æ ¹æ®æƒ…ç»ªçŠ¶æ€æä¾›å»ºè®®
        if emotion == 'negative':
            tips.append("ğŸ’¡ å…ˆè¡¨è¾¾åŒç†å¿ƒï¼Œä¸è¦æ€¥äºè§£é‡Šæˆ–è¾©è§£")
            tips.append("ğŸ”¥ ç”¨æˆ·æƒ…ç»ªè¾ƒä¸ºæ¿€åŠ¨ï¼Œä¿æŒå†·é™å’Œè€å¿ƒ")
            tips.append("ğŸ¯ å…³æ³¨è§£å†³é—®é¢˜ï¼Œè€Œä¸æ˜¯è®¨è®ºè´£ä»»")
        elif emotion == 'frustrated':
            tips.append("âš ï¸ ç”¨æˆ·å¯èƒ½æ„Ÿåˆ°æŒ«è´¥ï¼Œæä¾›æ¸…æ™°çš„è§£å†³è·¯å¾„")
            tips.append("ğŸ¤ ä¸»åŠ¨æ‰¿æ‹…ååŠ©è´£ä»»ï¼Œé™ä½ç”¨æˆ·å¿ƒç†è´Ÿæ‹…")
        
        # æ ¹æ®ç´§æ€¥ç¨‹åº¦æä¾›å»ºè®®
        if urgency == 'high':
            tips.append("ğŸš¨ é«˜ä¼˜å…ˆçº§å¤„ç†ï¼ŒåŠæ—¶å“åº”å’Œè·Ÿè¿›")
            tips.append("â° é¢„ä¼°å¤„ç†æ—¶é—´å¹¶åŠæ—¶åŒæ­¥è¿›å±•")
        elif urgency == 'critical':
            tips.append("ğŸ”´ ç´§æ€¥æƒ…å†µï¼Œè€ƒè™‘å‡çº§å¤„ç†æµç¨‹")
        
        # æ ¹æ®å¤æ‚ç¨‹åº¦æä¾›å»ºè®®
        if complexity == 'complex':
            tips.append("ğŸ§© é—®é¢˜è¾ƒä¸ºå¤æ‚ï¼Œå¯èƒ½éœ€è¦åˆ†æ­¥éª¤å¤„ç†")
            tips.append("ğŸ“‹ å»ºè®®è¯¦ç»†è®°å½•å¤„ç†è¿‡ç¨‹")
            tips.append("ğŸ‘¥ å¦‚éœ€è¦ï¼ŒåŠæ—¶å¯»æ±‚åŒäº‹æˆ–ä¸»ç®¡ååŠ©")
        
        # æ ¹æ®æ¶ˆæ¯ç±»å‹æä¾›å»ºè®®
        if message_type == 'complaint':
            tips.append("ğŸ“¢ æŠ•è¯‰å¤„ç†ï¼šå€¾å¬â†’ç†è§£â†’é“æ­‰â†’è§£å†³â†’è·Ÿè¿›")
            tips.append("ğŸ“ è¯¦ç»†è®°å½•æŠ•è¯‰å†…å®¹ï¼Œä¾¿äºåç»­æ”¹è¿›")
        elif message_type == 'question':
            tips.append("â“ ç¡®ä¿å®Œå…¨ç†è§£é—®é¢˜åå†å›ç­”")
            tips.append("ğŸ“š æä¾›å‡†ç¡®ä¿¡æ¯ï¼Œä¸ç¡®å®šæ—¶æŸ¥è¯åå›å¤")
        
        # ä¼šè¯ä¸Šä¸‹æ–‡å»ºè®®
        message_count = session_context.get('message_stats', {}).get('total_messages', 0)
        if message_count > 10:
            tips.append("ğŸ’¬ å¯¹è¯è¾ƒé•¿ï¼Œé€‚æ—¶æ€»ç»“è¦ç‚¹ç¡®ä¿ç†è§£ä¸€è‡´")
        
        return tips[:5]  # æœ€å¤šè¿”å›5ä¸ªå»ºè®®
    
    async def _get_llm_provider(self, tenant_id: UUID) -> BaseLLMProvider:
        """
        è·å–LLMæä¾›å•†
        
        Args:
            tenant_id: ç§Ÿæˆ·ID
            
        Returns:
            BaseLLMProvider: LLMæä¾›å•†å®ä¾‹
        """
        cache_key = f"suggestion_{tenant_id}"
        
        if cache_key in self._providers:
            return self._providers[cache_key]
        
        # ä½¿ç”¨OpenAIä½œä¸ºå»ºè®®æœåŠ¡çš„é»˜è®¤æä¾›å•†
        config = LLMConfig(
            model="gpt-3.5-turbo",
            temperature=0.7,  # é€‚ä¸­çš„åˆ›é€ æ€§
            max_tokens=1500
        )
        
        provider = OpenAIProvider(
            config=config,
            api_key="your-api-key",  # åº”è¯¥ä»é…ç½®ä¸­è·å–
            base_url="https://api.openai.com/v1"
        )
        
        self._providers[cache_key] = provider
        return provider 